# Molecular-HIV-Detection
AI Applications in Molecular Engineering: Predicting HIV Inhibition from Molecular Diagrams Using Schnet and Keras

**What did we complete?**
We implemented two models, one using the schnetpack packages, and another using a
Keras model. Our main goal was to compare the predictive power of these two models, which is shown in the notebook. We found that the schnet model that we used, containing 64 n_in and 64 epochs was able to predict 6291/6487 effectively. In the report regarding the keras models this will be compared to their success. In order to implement the Schnet model we used code provided by Dr. Ward along with the schnetpack documentation, the main parameter that was changed in this model had to do with changing the regression results to that of a classifier. The schnetpack package is normally used as a regression classifier, but the problem we had was one of classification meaning we had to choose a cutoff for the regressor that would best suit the model. Since nearly all values for the regressor came out between 0 and 1 (which was expected as these are the only values in the HIV_active data column) we iterated over all values 0 to 1 to the thousandth place and then chose the best value for our model.
For the Keras models, first the data was uploaded as saved png matplotlib figures of the images used in the Schnet models. This changed the size of the images from that used in Schnet and the images were resized later. We made three models - the first with a Conv2D, pooling, flatten, and two dense layers, and each consecutive model with an additional dense layer. A dense layer of unit size 1 and activation “sigmoid” was used as the last layer in each model since we are doing a binary classification. Each model was fitted over 20 epochs with 20 steps in each epoch. The RMSE of each model was taken as well and the performance of the three were compared in a graph at the end of the document.

**What did we learn?**
From the schnet analysis we learned the difficulty of data cleaning, and how hard it can
be to get the data in a working form. Much of our time there was spent debugging an issue related to NAs appearing in the dataset that caused the model training to be full of errors. We also learned the way to change regression results to classification results, just by creating and optimizing the cutoff of the regression data. In addition, we learned about the difficulty of training models and how it can take a while just to get a single model trained (it would take hours for any given model). We also learned some of the specifics of using AI packages, likely a useful skill in the future if attempting to do something specific either in materials science or another field. Finally, we learned about using PyTorch, as schnet is mainly implemented in PyTorch rather than TensorFlow.
From the Keras Modeling we also learned about the difficulties in image pre-processing and data formatting. We learned how to convert a displayed image in python to a .png file, store those files on a personal computer and categorize them, as well as recall them into a Keras image data generator to acquire data formatted for Keras model training. Furthermore, we learned of a
 
range of ways to upload data into Keras in the process including working with methods like .flow_from_directory and .flow_from_dataframe. We also learned about using different layers in CNNs and how it was useful to have our last layer follow a sigmoid function for the purposes of binary image classification.

**What would we do next?**
The next steps could involve further optimizing our models by varying additional
parameters. We were somewhat limited by the computing power of our local machines and if we had more time to utilize a computing cluster we could have tried more ambitious fitting parameters (more epochs, way more layers, etc). Additionally, the model would likely be much improved if we could have trained on more data initially. This would be another benefit to using a compute cluster as it would allow us to utilize batch jobs and powerful computers to more easily train on a much larger subset of the data (currently we only trained on 1000 and that was already taking quite a while). The biggest benefit of this would be that we could try many more parameters much faster and get a more optimal model. Finally, as noted in the schnet jupyter notebook, we attempted a couple of things that did not work out. This was discussed in further detail during the recording but basically after many trials of attempting to add a sigmoid layer, we did not manage to figure it out. The different methods attempted and associated errors are all in the jupyter notebook however, as to document the steps we tried and methods we attempted.
Next steps for the Keras models include using a larger subset of the HIV dataset and expanding our models to be trained over more epochs. In the end, the models were weak in that they were trained off of too small of a dataset and too skewed of a training dataset. These errors were realized later, but it was evident that a more diverse, larger, and equally distributed dataset is necessary for good results. Over 90% of the images in the training dataset were categorized as non-inhibitors, not giving the model much opportunity to learn patterns in inhibitors during fitting. In addition to expanding the dataset used, it will be useful to learn to upload images into Keras via direct numpy arrays that result from smiles strings turned into image arrays. This is because it will make it faster to upload the image data and categorize it since the images themselves will not need to be downloaded onto our personal devices, and instead can be generated within our code and uploaded straight into Keras. Lastly, we can add many more layers. Some of the Keras models we encountered in our research were vastly more complex than ours and produced much better results, even on smaller or less sophisticated datasets Expanding model complexity by adding and/or changing and experimenting further with which types of layers will be added can be a useful tool in this way.
